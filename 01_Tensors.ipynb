{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensors\n",
    "Some experiments on stuff that I found interesting or that was new to me."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skalar entries of `Tensor`s are `Tensor`s, as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first element:  tensor(1.)\n",
      "as float:       1.0\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(3)\n",
    "print(\"first element: \", a[0])\n",
    "print(\"as float:      \", float(a[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding dimensions (aka dirty `unsqueeze`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2])\n",
      "torch.Size([1, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
    "print(points.shape)\n",
    "print(points[None].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Einstein Summation (`torch.einsum`)\n",
    "A dense, powerful DSL to combine tensors. See [block post by Tim RockÃ¤schel](https://rockt.github.io/2018/04/30/einsum) for more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skalar product: $c = \\sum_i a_i * b_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(3)\n",
    "b = torch.ones(3)\n",
    "torch.einsum('i,i->', [a, b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor contraction with a transpose: $c_i = \\sum_{k} \\sum_{l} A_{ikl} B_{lk}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.5601, -3.2550])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.randn(2, 3, 4)\n",
    "B = torch.randn(4, 3)\n",
    "torch.einsum('ikl,lk->i', [A, B])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Tensors\n",
    "We can give names to the dimensions of a tensor. This is experimental but very promising, the names are considered when broadcasting and mismatched dimensions throw an error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naming tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "named_tensor:\n",
      "tensor([[1, 2],\n",
      "        [3, 4]], names=('rows', 'columns'))\n",
      "\n",
      "renamed_tensor:\n",
      "tensor([[[0., 0., 0.],\n",
      "         [0., 0., 0.]]], names=('C', 'H', 'W'))\n"
     ]
    }
   ],
   "source": [
    "named_tensor = torch.tensor([[1, 2], [3, 4]], names=['rows', 'columns'])\n",
    "print(f\"named_tensor:\\n{named_tensor}\\n\")\n",
    "\n",
    "unnamed_tensor = torch.zeros([1,2,3])\n",
    "renamed_tensor = unnamed_tensor.refine_names(..., 'C', 'H', 'W')\n",
    "print(f\"renamed_tensor:\\n{renamed_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mismatched dimensions throw an error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.ones(2,2, names=['H', 'W'])\n",
    "B = torch.ones(2,2, names=['W', 'H'])\n",
    "\n",
    "#A * B  # error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicitly re-aligning resolves the issue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.]], names=('H', 'W'))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A * B.align_as(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently Einstein summation is not supported with named tensors (drop names via `A.rename(None)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#torch.einsum('ij,ij->ij', [A, B])  # error\n",
    "torch.einsum('ij,ij->ij', [A.rename(None), B.rename(None)])  # fine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storage\n",
    "Tensors are wrappers around a storage in which the entries are stored flat. Higher dimensionalities are achieved by `shape` and `stride` attribute. This enables some-shaping operations to be efficient by simply producing new views into the same storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor:\n",
      "tensor([[1, 1, 1],\n",
      "        [2, 2, 2]])\n",
      "\n",
      "storage:\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 2\n",
      " 2\n",
      " 2\n",
      "[torch.storage.TypedStorage(dtype=torch.int64, device=cpu) of size 6]\n",
      "\n",
      "shape:\n",
      "torch.Size([2, 3])\n",
      "\n",
      "stride:\n",
      "(3, 1)\n",
      "\n",
      "transposed stride:\n",
      "(1, 3)\n",
      "\n",
      "tensor's storage == transposed tensor's storage:\n",
      "True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.tensor(((1,1,1), (2,2,2)))\n",
    "print(f\"tensor:\\n{tensor}\\n\")\n",
    "print(f\"storage:\\n{tensor.storage()}\\n\")\n",
    "print(f\"shape:\\n{tensor.shape}\\n\")\n",
    "print(f\"stride:\\n{tensor.stride()}\\n\")\n",
    "print(f\"transposed stride:\\n{tensor.t().stride()}\\n\")\n",
    "print(\"tensor's storage == transposed tensor's storage:\\n\"\n",
    "      f\"{tensor.storage().data_ptr() == tensor.t().storage().data_ptr()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This might lead to `Tensor`s that are non-contiguous. Some pytorch operations require contiguous memory layout which can be achieved with `.contiguous()` which returns a new `Tensor` with its own, contiguous storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor computation too slow?\n",
    "\n",
    "Have a look at [tensor comprehensions](https://pytorch.org/blog/tensor-comprehensions/) to conveniently write custom GPU code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch_DL-Ax2zpt-A",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
